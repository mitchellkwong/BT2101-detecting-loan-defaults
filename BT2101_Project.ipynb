{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T05: Koay Tze Min (A0188851N), Lai Yan Jean (A0190326J), Lee Jing Xuan (A0189467H), Mitchell Kwong (A0182695N), Valary Lim Wan Qian (A0190343L) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(tidyverse)\n",
    "library(Hmisc)\n",
    "library(corrplot)\n",
    "library(InformationValue)\n",
    "library(MASS)\n",
    "library(nnet)\n",
    "library(naivebayes)\n",
    "library(pracma)\n",
    "library(psych)\n",
    "library(CHAID)\n",
    "library(partykit)\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(ROSE)\n",
    "library(e1071)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read card.csv dataset\n",
    "data <- read.table('card.csv', sep=\",\", skip=2, header=FALSE)\n",
    "header1 <- scan('card.csv', sep=\",\", nlines=1, what=character())\n",
    "header2 <- scan('card.csv', sep=\",\", skip=1, nlines=1, what=character())\n",
    "\n",
    "# Add headers to the data\n",
    "header2[25] <- \"DEFAULT\"\n",
    "colnames(data) <- header2\n",
    "\n",
    "# Format columns\n",
    "data <- mutate_at(data, c(\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\", \"DEFAULT\"), as.factor)\n",
    "str(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the continuous and discrete data\n",
    "continuous_data <- select(data, LIMIT_BAL, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6,\n",
    "                          BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6,\n",
    "                          PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, DEFAULT)\n",
    "discrete_data <- select(data, SEX, EDUCATION, MARRIAGE, DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(continuous_data[, 1:20])\n",
    "continuous_data %>% describe()\n",
    "# There are no missing values in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test for Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Test for Significance\n",
    "tmat <- data.frame(tstat = numeric(), pvalue = numeric())\n",
    "for (i in 1:20) {\n",
    "  result <- t.test(continuous_data[,i] ~ continuous_data$DEFAULT, var.equal = TRUE)\n",
    "  tmat[i, 1] = result$statistic\n",
    "  tmat[i, 2] = result$p.value\n",
    "}\n",
    "tmat <- cbind(names(continuous_data[,1:20]), tmat)\n",
    "names(tmat) <- c(\"attribute\", \"t-statistic\", \"p-value\")\n",
    "tmat$significant <- ifelse(tmat$`p-value` < 0.05, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution: Limit Balance\n",
    "boxplot(continuous_data$LIMIT_BAL, xlab = \"Limit Balance\", ylab = \"NT Dollar\", col = \"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = LIMIT_BAL)) + geom_histogram(bins = 50, fill=\"darkslateblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution: Age\n",
    "boxplot(continuous_data$AGE, xlab = \"Age\", ylab = \"Years\", col = \"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = AGE)) + geom_histogram(bins = 50, fill=\"darkslateblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution: Payment Records\n",
    "boxplot(continuous_data[,3:8], xlab = \"Payment Records\", col = \"darkslateblue\")\n",
    "\n",
    "ggplot(continuous_data, aes(x = PAY_0)) + geom_histogram(bins = 10, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_2)) + geom_histogram(bins = 10, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_3)) + geom_histogram(bins = 10, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_4)) + geom_histogram(bins = 10, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_5)) + geom_histogram(bins = 10, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_6)) + geom_histogram(bins = 10, fill=\"darkslateblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution: Bill Amount\n",
    "boxplot(continuous_data[,9:14], xlab = \"Bill Amount\", ylab = \"NT Dollar\", col = \"darkslateblue\")\n",
    "\n",
    "ggplot(continuous_data, aes(x = BILL_AMT1)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = BILL_AMT2)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = BILL_AMT3)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = BILL_AMT4)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = BILL_AMT5)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = BILL_AMT6)) + geom_histogram(bins = 30, fill=\"darkslateblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution: Payment Amount\n",
    "boxplot(continuous_data[,15:20], xlab = \"Payment Amount\", ylab = \"NT Dollar\", col = \"darkslateblue\")\n",
    "\n",
    "ggplot(continuous_data, aes(x = PAY_AMT1)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_AMT2)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_AMT3)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_AMT4)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_AMT5)) + geom_histogram(bins = 30, fill=\"darkslateblue\")\n",
    "ggplot(continuous_data, aes(x = PAY_AMT6)) + geom_histogram(bins = 30, fill=\"darkslateblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "corr.result <- rcorr(as.matrix(continuous_data)) # get mutual correlation\n",
    "corrplot.mixed(corr.result$r, lower.col = \"black\", number.cex = .5, tl.cex = .45, tl.col = \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to Factor Variable\n",
    "data$SEX = as.factor(data$SEX)\n",
    "data$EDUCATION = as.factor(data$EDUCATION)\n",
    "data$MARRIAGE = as.factor(data$MARRIAGE)\n",
    "data$DEFAULT = as.factor(data$DEFAULT)\n",
    "\n",
    "discrete_data <- select(train.data, SEX, EDUCATION, MARRIAGE, DEFAULT)\n",
    "View(discrete_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_table <- table(discrete_data$SEX)\n",
    "edu_table <- table(discrete_data$EDUCATION)\n",
    "marriage_table <- table(discrete_data$MARRIAGE)\n",
    "default_table <- table(discrete_data$DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart: Sex\n",
    "ylim <- c(0, 1.1*max(table(discrete_data$SEX)))\n",
    "bar2 = barplot(table(discrete_data$SEX), names.arg = c(\"Male\",\"Female\"), main = \"Sex\", col = \"darkslateblue\", ylim = ylim)\n",
    "text(x = bar2, y = table(discrete_data$SEX), label = table(discrete_data$SEX), pos = 3, cex = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart: Education\n",
    "ylim <- c(0, 1.1*max(table(discrete_data$EDUCATION)))\n",
    "bar3 = barplot(table(discrete_data$EDUCATION),main = \"Education level\", col = \"darkslateblue\",ylim = ylim)\n",
    "text(x = bar3, y = table(discrete_data$EDUCATION), label = table(discrete_data$EDUCATION), pos = 3, cex = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart: Marriage\n",
    "ylim <- c(0, 1.1*max(table(discrete_data$MARRIAGE)))\n",
    "bar4 = barplot(table(discrete_data$MARRIAGE), main = \"Marital Status\", col = \"darkslateblue\", ylim = ylim)\n",
    "text(x = bar4, y = table(discrete_data$MARRIAGE), label = table(discrete_data$MARRIAGE), pos = 3, cex = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart: Default (Target Variable)\n",
    "ylim <- c(0, 1.1*max(table(discrete_data$DEFAULT)))\n",
    "bar1 = barplot(table(discrete_data$DEFAULT), main = \"Default\", names.arg = c(\"No\",\"Yes\"), col = \"darkslateblue\",ylim = ylim)\n",
    "text(x = bar1, y = table(discrete_data$DEFAULT), label = table(discrete_data$DEFAULT), pos = 3, cex = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq.test(discrete_data$SEX, discrete_data$DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq.test(discrete_data$EDUCATION, discrete_data$DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq.test(discrete_data$MARRIAGE, discrete_data$DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and testing sets\n",
    "set.seed(123)\n",
    "n <- length(data$DEFAULT)\n",
    "index <- 1:nrow(data)\n",
    "testindex <- sample(index, trunc(2 * n) / 3)\n",
    "test.data <- data[testindex, ]\n",
    "train.data <- data[-testindex, ]\n",
    "\n",
    "print(paste('shape(Xtr) = (', nrow(train.data), ',', ncol(train.data), ')'))\n",
    "print(paste('shape(Xte) = (', nrow(test.data), ',', ncol(test.data), ')'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to csv\n",
    "write.csv(train.data, paste(path, 'data', 'train.csv', sep='/'), row.names=FALSE)\n",
    "write.csv(test.data, paste(path, 'data', 'test.csv', sep='/'), row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Variables\n",
    "The next few sections were coded in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from Python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and initialise column names\n",
    "df = pd.read_csv(f'{path}/data/card.csv', skiprows=1)\n",
    "\n",
    "ID = ['ID']\n",
    "continuous = ['LIMIT_BAL', 'AGE']\n",
    "categories = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "paycols = ['PAY_'+i for i in '023456']\n",
    "billcols = [col+i for col in ('BILL_AMT', 'PAY_AMT') for i in '123456']\n",
    "target = ['default payment next month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining code to encode\n",
    "def encode(df):\n",
    "  # work on a fresh copy\n",
    "  cleaned = df.copy()\n",
    "\n",
    "  # Categorical data\n",
    "  cleaned['SEX_male'] = np.where(cleaned['SEX'] == 1, 1, 0)\n",
    "\n",
    "  cleaned['EDUCATION_gradSch'] = np.where(cleaned['EDUCATION'] == 1, 1, 0)\n",
    "  cleaned['EDUCATION_university'] = np.where(cleaned['EDUCATION'] == 2, 1, 0)\n",
    "  cleaned['EDUCATION_highSch'] = np.where(cleaned['EDUCATION'] == 3, 1, 0)\n",
    "\n",
    "  cleaned['MARRIAGE_married'] = np.where(cleaned['MARRIAGE'] == 1, 1, 0)\n",
    "  cleaned['MARRIAGE_single'] = np.where(cleaned['MARRIAGE'] == 2, 1, 0)\n",
    "\n",
    "  # Mixed data\n",
    "  delay = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "  for col in paycols:\n",
    "    cleaned[col+'_unknown0'] = np.where(cleaned[col] == 0, 1, 0)\n",
    "    cleaned[col+'_unknown2'] = np.where(cleaned[col] == -2, 1, 0)\n",
    "    cleaned[col+'_payDuly'] = np.where(cleaned[col] == -1, 1, 0)\n",
    "    cleaned[col+'_value'] = cleaned[col].isin(delay) * cleaned[col]\n",
    "\n",
    "  new_paycols = sum(([col+'_unknown0', col+'_unknown2', col+'_payDuly', col+'_value'] for col in paycols), [])\n",
    "\n",
    "  orderedcols = [\n",
    "      *ID,\n",
    "      'LIMIT_BAL',\n",
    "      'SEX_male', \n",
    "      'EDUCATION_gradSch', 'EDUCATION_university','EDUCATION_highSch',\n",
    "      'MARRIAGE_married', 'MARRIAGE_single',\n",
    "      'AGE',\n",
    "      *new_paycols,\n",
    "      *billcols,\n",
    "      *target,\n",
    "  ]\n",
    "\n",
    "  colnames = [\n",
    "      *ID,\n",
    "      'LIMIT_BAL',\n",
    "      'SEX_male', \n",
    "      'EDUCATION_gradSch', 'EDUCATION_university','EDUCATION_highSch',\n",
    "      'MARRIAGE_married', 'MARRIAGE_single',\n",
    "      'AGE',\n",
    "      *new_paycols,\n",
    "      *billcols,\n",
    "      'DEFAULT',\n",
    "  ]\n",
    "\n",
    "  export = cleaned[orderedcols]\n",
    "  export.columns = colnames\n",
    "\n",
    "  return export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding train set and exporting it\n",
    "train = pd.read_csv(f'{path}/data/test.csv')\n",
    "encode(train).to_csv(f'{path}/data/encoded_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding test set and exporting it\n",
    "test = pd.read_csv(f'{path}/data/test.csv')\n",
    "encode(train).to_csv(f'{path}/data/encoded_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scale = continuous + billcols + [f'PAY_{i}_value' for i in '023456']\n",
    "\n",
    "train = pd.read_csv(f'{path}/data/encoded_train.csv')\n",
    "test = pd.read_csv(f'{path}/data/encoded_test.csv')\n",
    "\n",
    "scaled_train = (train - train.mean()) / train.std()\n",
    "scaled_test = (test - train.mean()) / train.std()\n",
    "\n",
    "export_train = train.copy()\n",
    "export_test = test.copy()\n",
    "\n",
    "export_train[to_scale] = scaled_train[to_scale]\n",
    "export_test[to_scale] = scaled_test[to_scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to csv\n",
    "export_train.to_csv(f'{path}/data/card_train.csv', index=False)\n",
    "export_test.to_csv(f'{path}/data/card_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Data by Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = pd.read_csv(f'{path}/data/card_train.csv')\n",
    "\n",
    "counts = skewed.groupby('DEFAULT')['DEFAULT'].count()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = skewed[skewed['DEFAULT']==0]\n",
    "pos = skewed[skewed['DEFAULT']==1]\n",
    "\n",
    "train_under = pd.concat([\n",
    "    neg.sample(counts.min()),\n",
    "    pos,\n",
    "])\n",
    "\n",
    "train_over = pd.concat([\n",
    "    skewed,\n",
    "    pos.sample(counts.max() - counts.min(), replace=True)\n",
    "])\n",
    "\n",
    "print(\n",
    "    'After undersampling:',\n",
    "    train_under.groupby('DEFAULT')['DEFAULT'].count(),\n",
    "    '',\n",
    "    'After oversampling:',\n",
    "    train_over.groupby('DEFAULT')['DEFAULT'].count(),\n",
    "    sep='\\n',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle rows\n",
    "train_under.sample(len(train_under), replace=False)\n",
    "train_over.sample(len(train_over), replace=False)\n",
    "\n",
    "train_under.to_csv(f'{path}/data/card_train_undersample.csv', index=False)\n",
    "train_over.to_csv(f'{path}/data/card_train_oversample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read oversampled train data\n",
    "initial.train.data <- read.csv(\"card_train_oversample.csv\")\n",
    "# Remove the ID column\n",
    "initial.train.data <- initial.train.data[, -1]\n",
    "\n",
    "# Read test data\n",
    "test.data <- read.csv(\"test_data.csv\")\n",
    "# Remove the ID column\n",
    "test.data <- test.data[, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read full train dataset that is unbalanced\n",
    "full.train.data <- read.csv(\"card_train.csv\")\n",
    "# Remove the ID column\n",
    "full.train.data <- full.train.data[,-1]\n",
    "# Change decision variable to factor\n",
    "full.train.data$DEFAULT = as.factor(full.train.data$DEFAULT)\n",
    "View(full.train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train-validation sets\n",
    "folds <- cut(seq(1,nrow(full.train.data)), breaks = 10, labels = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Logistic Regression (without Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise f1 vector\n",
    "f1 <- NULL\n",
    "\n",
    "# Perform 10 fold cross validation\n",
    "for (i in 1:10) {\n",
    "  # Segment data by fold using the which() function\n",
    "  val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "  \n",
    "  # Split train and test data\n",
    "  val.data <- full.train.data[val.indexes, ]\n",
    "  train.data <- full.train.data[-val.indexes, ]\n",
    "  \n",
    "  # Generate the model\n",
    "  model = glm(DEFAULT~., data = train.data, family = binomial) \n",
    "  \n",
    "  # Get optimal cut off\n",
    "  lr.pred <- predict(model, newdata = train.data[,-45],type=\"response\")\n",
    "  optcut <- optimalCutoff(train.data$DEFAULT, lr.pred, optimiseFor=\"Both\")\n",
    "  \n",
    "  # Predict on validation data using optimal cut off from train data \n",
    "  lr.pred.val <- predict(model, newdata = val.data[,-45],type=\"response\")\n",
    "  lr.binpred <- ifelse(lr.pred.val < optcut ,0,1)\n",
    "  \n",
    "  # Calculate f1 score\n",
    "  recall = sensitivity(actuals = val.data$DEFAULT, predictedScores = lr.binpred, threshold = optcut)\n",
    "  prec = precision(actuals = val.data$DEFAULT, predictedScores = lr.binpred, threshold = optcut)\n",
    "  F1.score = 2 *(recall * prec)/(recall + prec)\n",
    "  f1[i] <- F1.score\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Akaike Information Criterion (AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise full model for backward feature selection\n",
    "fullmodel = glm(DEFAULT ~ ., data = full.train.data, family = binomial) \n",
    "\n",
    "# Optimising model using AIC\n",
    "lr.model <- stepAIC(fullmodel, direction = \"backward\", trace = FALSE)\n",
    "\n",
    "# Initialise f1 vector\n",
    "f1.aic <- NULL\n",
    "\n",
    "# Creating formula\n",
    "formula = as.formula(paste(\"DEFAULT ~ \", paste(row.names(as.matrix(coef(lr.model)))[-1], collapse = \"+\")))\n",
    "\n",
    "# Perform 10 fold cross validation\n",
    "for (i in 1:10) {\n",
    "  # Segment data by fold using the which() function\n",
    "  val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "  \n",
    "  # Split train and test data\n",
    "  val.data <- full.train.data[val.indexes, ]\n",
    "  train.data <- full.train.data[-val.indexes, ]\n",
    "  \n",
    "  # Generate the model\n",
    "  model = glm(formula, data = train.data, family = binomial) \n",
    "  \n",
    "  # Get optimal cut off\n",
    "  lr.pred <- predict(model, newdata = train.data[,-45],type=\"response\")\n",
    "  optcut1 <- optimalCutoff(train.data$DEFAULT, lr.pred, optimiseFor=\"Both\")\n",
    "  \n",
    "  # Predict on validation data using optimal cut off from train data \n",
    "  lr.pred.val <- predict(model, newdata = val.data[,-45],type=\"response\")\n",
    "  lr.binpred <- ifelse(lr.pred.val < optcut1 ,0,1)\n",
    "  \n",
    "  # Calculate f1 score\n",
    "  recall = sensitivity(actuals = val.data$DEFAULT, predictedScores = lr.binpred, threshold = optcut1)\n",
    "  prec = precision(actuals = val.data$DEFAULT, predictedScores = lr.binpred, threshold = optcut1)\n",
    "  F1.score = 2 *(recall * prec)/(recall + prec)\n",
    "  f1.aic[i] <- F1.score\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Feature Selection (using p-Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise full model for backward selection\n",
    "fullmodel = glm(DEFAULT ~ ., data = full.train.data, family = binomial) \n",
    "model.p = fullmodel\n",
    "formula.p = NULL\n",
    "\n",
    "# Initialise the p-values from the model for iteration\n",
    "p.values = as.matrix(summary(model.p)$coefficients[,4])\n",
    "p.values = as.matrix(p.values[rownames(p.values) != \"(Intercept)\",])\n",
    "\n",
    "# Iteratively remove variables with p-values > 0.05 until all p-values < 0.05\n",
    "while (any(p.values > 0.05)){\n",
    "  # Removing variable with the maximum p-value\n",
    "  p.values = subset(p.values, p.values!=max(p.values))\n",
    "  variables = row.names(p.values)\n",
    "  variables\n",
    "  \n",
    "  # Creating formula\n",
    "  formula.p = as.formula(paste(\"DEFAULT ~ \", paste(variables, collapse = \"+\")))\n",
    "  \n",
    "  # Create model\n",
    "  model.p = glm(formula.p, data = full.train.data, family = binomial)\n",
    "  \n",
    "  # Retrieve and update variables and p-values for the next iteration\n",
    "  p.values = as.matrix(summary(model.p)$coefficients[,4])\n",
    "  p.values = as.matrix(p.values[rownames(p.values) != \"(Intercept)\",])\n",
    "}\n",
    "\n",
    "# Initialise F1 score\n",
    "f1.p <- NULL\n",
    "\n",
    "# Perform 10 fold cross validation\n",
    "for (i in 1:10) {\n",
    "  # Segment data by fold using the which() function\n",
    "  val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "  \n",
    "  # Split train and test data\n",
    "  val.data <- full.train.data[val.indexes, ]\n",
    "  train.data <- full.train.data[-val.indexes, ]\n",
    "  \n",
    "  # Generate the model  \n",
    "  model.p = glm(formula.p, data = train.data, family = binomial) \n",
    "  \n",
    "  # Get optimal cut off\n",
    "  lr.pred <- predict(model.p, newdata = train.data[,-45],type=\"response\")\n",
    "  optcut2 <- optimalCutoff(train.data$DEFAULT, lr.pred, optimiseFor=\"Both\")\n",
    "  \n",
    "  # Predict on validation data using optimal cut off from train data \n",
    "  lr.pred.val <- predict(model.p, newdata = val.data[,-45],type=\"response\")\n",
    "  lr.binpred <- ifelse(lr.pred.val < optcut2 ,0,1)\n",
    "  \n",
    "  # Calculate f1 score\n",
    "  recall = sensitivity(actuals = val.data$DEFAULT, predictedScores = lr.binpred, threshold = optcut2)\n",
    "  prec = precision(actuals = val.data$DEFAULT, predictedScores = lr.binpred, threshold = optcut2)\n",
    "  F1.score = 2 *(recall * prec)/(recall + prec)\n",
    "  f1.p[i] <- F1.score\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Different Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix consisting of the results of the cross validation (mean F1 score)\n",
    "mat = as.matrix(c(mean(f1), mean(f1.aic),mean(f1.p)))\n",
    "colnames(mat) = c(\"Mean F1 score\")\n",
    "rownames(mat) = c(\"Full model\",\"AIC\",\"Iterative\")\n",
    "# Best model found is AIC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building overall regression model using best model from validation\n",
    "pred <- predict(lr.model, newdata = full.train.data[,-45],type=\"response\")\n",
    "# Get optimal cut off\n",
    "optcut.final <- optimalCutoff(full.train.data$DEFAULT, pred, optimiseFor=\"Both\")\n",
    "# Save Model\n",
    "saveRDS(lr.model, file = \"model_lr.rda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree\n",
    "We first tried to build a basic classification tree model using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpart (Recursive Partitioning) Algorithm\n",
    "# Based on CART modelling which uses GINI index\n",
    "\n",
    "# Grow tree\n",
    "cart.tree1 <- rpart(`DEFAULT` ~., data=train.data[-1], method = 'class')\n",
    "# Display the results\n",
    "printcp(cart.tree1) \n",
    "# Detailed summary of splits\n",
    "summary(cart.tree1) \n",
    "# Visualise cross-validation results\n",
    "plotcp(cart.tree1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Tree\n",
    "plot(cart.tree1, main = \"Classification Tree\")\n",
    "text(cart.tree1, use.n = TRUE, xpd = TRUE, cex = 0.8)\n",
    "\n",
    "# Postscript Plot\n",
    "post(cart.tree1, file = \"tree.ps\", title = \"Classification Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed evaluation on the train set using the initial model built, pruned the tree then performed an evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation - Train Set\n",
    "train.pred1 <- predict(cart.tree1, type = \"class\")\n",
    "train.cm <- table(pred=train.pred1, actual=train.data$DEFAULT)\n",
    "train.cm.recall <- train.cm[2,2] / (train.cm[2,2] + train.cm[1,2])\n",
    "train.cm.precision <- train.cm[2,2] / (train.cm[2,1] + train.cm[2,2])\n",
    "train.cm.f1 <- (2 * train.cm.precision * train.cm.recall) / (train.cm.precision + train.cm.recall)\n",
    "\n",
    "train.cm.f1 #0.6229132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune Tree\n",
    "cart.tree1.pruned = prune(cart.tree1, cp = 0.0100)\n",
    "# Choose the one that has the lowest xerror\n",
    "plot(cart.tree1.pruned, main = \"Classification Tree Pruned\")\n",
    "text(cart.tree1.pruned, cex = 0.9, xpd = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Analysis - Test Set\n",
    "test.pred1 <- predict(cart.tree1.pruned, test.data[, c(1:45)], type = \"class\")\n",
    "test.cm <- table(pred=test.pred1, actual=test.data$DEFAULT)\n",
    "test.cm.recall <- test.cm[2,2] / (test.cm[2,2] + test.cm[1,2])\n",
    "test.cm.precision <- test.cm[2,2] / (test.cm[2,1] + test.cm[2,2])\n",
    "test.cm.f1 <- (2 * test.cm.precision * test.cm.recall) / (test.cm.precision + test.cm.recall)\n",
    "\n",
    "test.cm.f1 #0.5132392"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Validation\n",
    "We tuned the model using different values of cp, using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different cp (complexity control parameter) to craft CART Tree\n",
    "tryCP <- c(0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001)\n",
    "# Create dataframe to store the result\n",
    "result <- data.frame(matrix(ncol=3, nrow=8))\n",
    "rCols <- c(\"cp\", \"val.accuracy\", \"val.f1\")\n",
    "colnames(result) <- rCols\n",
    "\n",
    "# Create 10 equally size folds\n",
    "initial.train.data <- train.data\n",
    "folds <- cut(seq(1,nrow(initial.train.data)), breaks = 10, labels = FALSE)\n",
    "\n",
    "index <- 1\n",
    "\n",
    "for (i in tryCP) {\n",
    "  # Initialise Accuracy and F1\n",
    "  val.accuracy <- 0\n",
    "  val.f1 <- 0\n",
    "  \n",
    "  # Perform 10 fold cross validation\n",
    "  for (k in 1:10) {\n",
    "    # Segment data by fold using the which() function\n",
    "    val.indexes <- which(folds == k, arr.ind = TRUE)\n",
    "    \n",
    "    # Split train and test data\n",
    "    val.data <- initial.train.data[val.indexes, ]\n",
    "    train.data <- initial.train.data[-val.indexes, ]\n",
    "    \n",
    "    # Craft Decision Tree\n",
    "    cart.tree <- rpart(`DEFAULT` ~., data = train.data[-1], method = 'class',\n",
    "                       control = rpart.control(cp = i))\n",
    "    \n",
    "    # Prune Tree\n",
    "    cart.tree.pruned = prune(cart.tree, \n",
    "                             cp = i)\n",
    "\n",
    "    # Predict on Validation Set\n",
    "    val.pred <- predict(cart.tree.pruned, val.data[, c(1:44)], type = \"class\")\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    CM <- table(pred = val.pred, actual = val.data$DEFAULT)\n",
    "    \n",
    "    # Metrics\n",
    "    TP <- CM[2,2]\n",
    "    TN <- CM[1,1]\n",
    "    FP <- CM[2,1]\n",
    "    FN <- CM[1,2]\n",
    "\n",
    "    recall <- TP / (TP + FN)\n",
    "    precision <- TP / (FP + TP)\n",
    "    \n",
    "    # Accuracy Analysis - Validation Set\n",
    "    accuracy <- (TP + TN) / sum(CM)\n",
    "    val.accuracy <- val.accuracy + accuracy\n",
    "    \n",
    "    # F1 Analysis - Validation Set\n",
    "    f1 <- (2 * precision * recall) / (precision + recall)\n",
    "    val.f1 <- val.f1 + f1\n",
    "  }\n",
    "  \n",
    "  # Get average Accuracy\n",
    "  val.accuracy <- val.accuracy / 10\n",
    "  \n",
    "  # Get average F1 score\n",
    "  val.f1 <- val.f1 / 10\n",
    "\n",
    "  # Add to results\n",
    "  res <- c(i,\n",
    "           val.accuracy,\n",
    "           val.f1)\n",
    "  \n",
    "  result[index, ] <- res\n",
    "  index <- index + 1\n",
    "}\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model Training\n",
    "We built our final model based on the best cp value obtained from model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Training\n",
    "# grow tree\n",
    "cart.tree.final <- rpart(`DEFAULT` ~., data=train.data[-1], method = 'class',\n",
    "                         control = rpart.control(cp = 0.0001))\n",
    "# Display the results\n",
    "printcp(cart.tree.final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Splits\n",
    "# Detailed summary of splits\n",
    "summary(cart.tree.final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Validation\n",
    "# Visualise cross-validation results\n",
    "plotcp(cart.tree.final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Tree\n",
    "plot(cart.tree.final, main = \"Classification Tree\")\n",
    "text(cart.tree.final, use.n = TRUE, xpd = TRUE, cex = 0.8)\n",
    "\n",
    "# Postscript Plot\n",
    "post(cart.tree.final, file = \"tree.ps\", title = \"Classification Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune Tree\n",
    "cart.tree.final.pruned = prune(cart.tree.final, cp = 0.0001)\n",
    "# Plot Tree\n",
    "plot(cart.tree.final.pruned, main = \"Classification Tree Pruned\")\n",
    "text(cart.tree.final.pruned, cex = 0.9, xpd = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models\n",
    "saveRDS(cart.tree.final, file = \"model_CARTtree.rda\")\n",
    "saveRDS(cart.tree.final.pruned, file = \"model_CARTtreePruned.rda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Initial Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "rf.model <- randomForest(`DEFAULT`~., data=train.data, importance=TRUE)\n",
    "\n",
    "print(rf.model)\n",
    "# mtry: Number of variables randomly sampled as candidates at each split.\n",
    "# ntree: Number of trees to grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Evaluation Function\n",
    "evaluate <- function(prediction, decision) {\n",
    "  table <- table(pred=prediction, actual=decision)\n",
    "  print(table)\n",
    "  \n",
    "  #Metrics\n",
    "  TP <- table[2,2]\n",
    "  TN <- table[1,1]\n",
    "  FP <- table[2,1]\n",
    "  FN <- table[1,2]\n",
    "  \n",
    "  accuracy <- (TP + TN) / sum(table)\n",
    "  precision <- TP / (TP + FP)\n",
    "  recall <- TP / (TP + FN)\n",
    "  F1 <- (2 * precision * recall) / (precision + recall)\n",
    "  \n",
    "  cat('- Accuracy', accuracy, '\\n')\n",
    "  cat('- Precision', precision, '\\n')\n",
    "  cat('- Recall', recall, '\\n')\n",
    "  cat('- F1 Score', F1, '\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Train Data\n",
    "train.data$rf.prediction <- predict(rf.model, train.data[, c(1:44)])\n",
    "evaluate(train.data$rf.prediction, train.data$DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Test Data\n",
    "test.data$rf.prediction <- predict(rf.model, test.data[, c(1:44)])\n",
    "evaluate(test.data$rf.prediction, test.data$DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Importance\n",
    "importance(rf.model)\n",
    "varImpPlot(rf.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up results dataframe to store results\n",
    "result <- data.frame(matrix(ncol=4, nrow=20))\n",
    "rCols <- c(\"ntree\", \"mtry\", \"val.accuracy\", \"val.f1\")\n",
    "colnames(result) <- rCols\n",
    "\n",
    "# Split train into train-validation (80-20)\n",
    "initial.train.data <- train.data\n",
    "set.seed(123)\n",
    "n = length(initial.train.data$DEFAULT)\n",
    "index <- 1:nrow(initial.train.data)\n",
    "valindex <- sample(x = index, size = trunc(n * 0.2))\n",
    "val.data <- initial.train.data[valindex,]\n",
    "train.data <- initial.train.data[-valindex,]\n",
    "\n",
    "index <- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Tuning\n",
    "for (ntree in seq(100, 2000, 100)) {\n",
    "  set.seed(123);\n",
    "  rf.model.tune1 <- tuneRF(x = train.data[, c(1:44)], y = train.data$DEFAULT,\n",
    "                           stepFactor = 1.5, improve = 1e-5, ntree=ntree)\n",
    "  \n",
    "  # Find optimal mtry\n",
    "  optimal.mtry <- rf.model.tune1[which.min(rf.model.tune1[,\"OOBError\"]),\"mtry\"]\n",
    "  \n",
    "  # Build model\n",
    "  set.seed(123)\n",
    "  rf.model <- randomForest(`DEFAULT`~., data=train.data, importance=TRUE,\n",
    "                           mtry = optimal.mtry, ntree = ntree)\n",
    "  \n",
    "  # Predict on Validation Set\n",
    "  # Exclude decision column\n",
    "  val.pred <- predict(rf.model, val.data[, c(1:44)])\n",
    "\n",
    "  # Create confusion matrix\n",
    "  CM <- table(pred = val.pred, actual = val.data$DEFAULT)\n",
    "    \n",
    "  # Metrics\n",
    "  TP <- CM[2,2]\n",
    "  TN <- CM[1,1]\n",
    "  FP <- CM[2,1]\n",
    "  FN <- CM[1,2]\n",
    "\n",
    "  recall <- TP / (TP + FN)\n",
    "  precision <- TP / (TP + FP)\n",
    "    \n",
    "  # Accuracy Analysis - Validation Set\n",
    "  val.accuracy <- (TP + TN) / sum(CM)\n",
    "    \n",
    "  # F1 Analysis - Validation Set\n",
    "  val.f1 <- (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "  # Add to results\n",
    "  res <- c(ntree, optimal.mtry, val.accuracy, val.f1)\n",
    "  result[index, ] <- res\n",
    "  index <- index + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)\n",
    "# Sort result in decreasing order based on F1, then accuracy\n",
    "sorted <- result[order(-result$val.f1, -result$val.accuracy), ]\n",
    "print(sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model on entire train set\n",
    "# Choose ntree = 1600, mtry = 28\n",
    "rf.model.final <- randomForest(`DEFAULT`~., data=train.data, importance=TRUE, \n",
    "                               ntree = 1600, mtry = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model on train set\n",
    "train.pred <- predict(rf.model.final, train.data[, c(1:44)])\n",
    "evaluate(train.pred, train.data$DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model on test set\n",
    "test.pred <- predict(rf.model.final, test.data[, c(1:44)])\n",
    "evaluate(test.pred, test.data$DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "saveRDS(rf.model.final, file = \"model_rf.rda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "grid.search <- function(model.func, data, param.grid, ...) {\n",
    "    # Initialize result table\n",
    "    n.metrics <- length(list(...))\n",
    "    metrics <- data.frame(matrix(nrow=0, ncol=2*n.metrics)) # x2 for train and validation\n",
    "    \n",
    "    # k-fold cross validation\n",
    "    k <- 10\n",
    "    data <- data %>% mutate(fold=cut(seq(1,nrow(data)), breaks = k, labels = FALSE))\n",
    "\n",
    "    # Evaluate each parameter combination\n",
    "    n.params <- nrow(param.grid)\n",
    "    for (i in 1:n.params) {\n",
    "        \n",
    "        # Temp results table\n",
    "        metrics.fold <- data.frame(matrix(nrow=0, ncol=2*n.metrics))\n",
    "\n",
    "        # k fold cross validation\n",
    "        for (j in 1:k) {\n",
    "            # Split train and test data\n",
    "            train.data <- data %>% filter(fold!=j)\n",
    "            valid.data <- data %>% filter(fold==j)\n",
    "\n",
    "            # Fit model\n",
    "            params.data <- list(data=train.data)\n",
    "            params.grid <- param.grid %>% slice(i) %>% flatten()\n",
    "            params <- append(params.data, params.grid)\n",
    "            model.fit <- do.call(model.func, params)\n",
    "\n",
    "            # Model predictions\n",
    "            train.actual <- train.data$DEFAULT\n",
    "            train.pred <- predict(model.fit, type='prob')[,2]\n",
    "            \n",
    "            valid.actual <- valid.data$DEFAULT\n",
    "            valid.pred <- predict(model.fit, newdata = select(valid.data, -DEFAULT), type='prob')[,2]\n",
    "            \n",
    "            # Evaluate metrics\n",
    "            train.metrics <- list(...) %>% sapply((function(f) f(train.actual, train.pred)))\n",
    "            valid.metrics <- list(...) %>% sapply((function(f) f(valid.actual, valid.pred)))\n",
    "            metrics.fold <- rbind(metrics.fold, append(train.metrics, valid.metrics))\n",
    "        }\n",
    "        \n",
    "        \n",
    "        names(metrics.fold) <- c(paste('training', names(list(...)), sep='.'),\n",
    "                                 paste('validation', names(list(...)), sep='.'))\n",
    "        metrics <- rbind(metrics, metrics.fold %>% summarise_all(mean))\n",
    "    }\n",
    "\n",
    "    # Return metrics associated with each parameter combination\n",
    "    names(metrics) <- c(paste('training', names(list(...)), sep='.'), \n",
    "                        paste('validation', names(list(...)), sep='.'))\n",
    "  \n",
    "    return(cbind(param.grid, metrics))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-build a model from the grid search\n",
    "build.model <- function(model.func, data, params) {\n",
    "    # Train on training set\n",
    "    params.data <- list(data=data)\n",
    "    params.grid <- params\n",
    "    params <- append(params.data, params.grid)\n",
    "    model.fit <- do.call(model.func, params)\n",
    "    \n",
    "    # return fitted model\n",
    "    return(model.fit)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "getacc <- function(y, ypred) {\n",
    "    # ypred is the predicted probability\n",
    "    mean(y == ifelse(ypred > 0.5, 1, 0))\n",
    "}\n",
    "\n",
    "\n",
    "getAUC <- function(y, ypred) {\n",
    "    M <- plotROC(y, ypred, returnSensitivityMat = TRUE)\n",
    "    x <- M[,1]\n",
    "    y <- M[,2]\n",
    "\n",
    "    W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "    H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "    \n",
    "    return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "getF1 <- function(y, ypred) {\n",
    "    xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                  factor(ypred > 0.5, levels=c(TRUE, FALSE)))\n",
    "    \n",
    "    precision <- xtab[1,1] / sum(xtab[,1])\n",
    "    recall <- xtab[1,1] / sum(xtab[1,])\n",
    "\n",
    "    return(2 * (precision * recall) / (precision + recall))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default args\n",
    "prior.default = list(NULL)\n",
    "laplace.default = list(0)\n",
    "usekernel.default = list(FALSE)\n",
    "usepoisson.default = list(FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate args\n",
    "xtab <- table(data.original$DEFAULT)\n",
    "xtab <- xtab / sum(xtab)\n",
    "\n",
    "prior = list(xtab, NULL)\n",
    "laplace <- logseq(10e-5, 10e5, n=10)\n",
    "usekernel = list(TRUE, FALSE)\n",
    "usepoisson = list(TRUE, FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes model\n",
    "func = function(...) {naive_bayes(DEFAULT~., ...)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minor data processing and feature selection using results from EDA\n",
    "data.reduced <- data.balanced %>%\n",
    "    select(LIMIT_BAL, PAY_AMT1, AGE, \n",
    "            PAY_0_value, PAY_0_payDuly, PAY_0_unknown0, PAY_0_unknown2,\n",
    "            EDUCATION_gradSch, EDUCATION_university, EDUCATION_highSch, \n",
    "            MARRIAGE_married, MARRIAGE_single,\n",
    "            SEX_male, DEFAULT) %>%\n",
    "    mutate(DEFAULT=as.factor(DEFAULT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior Probabilities\n",
    "We tried to supply prior probabilities from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <-  grid.search(\n",
    "    model.func = func,\n",
    "    data = data.reduced,\n",
    "    param.grid = expand.grid(\n",
    "        prior = prior, \n",
    "        laplace = laplace.default, \n",
    "        usekernel = usekernel.default, \n",
    "        usepoisson = usepoisson.default\n",
    "    ),\n",
    "\n",
    "    # Metrics\n",
    "    AUC = getAUC, \n",
    "    acc = getacc\n",
    ")\n",
    "\n",
    "# Arrange model params by training accuracy\n",
    "results %>% arrange(desc(validation.acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace\n",
    "We search for a good laplace value over a logarithmic scale. This can be modified to iteratively narrow the range of the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <-  grid.search(\n",
    "    model.func = func,\n",
    "    data = data.reduced,\n",
    "    param.grid = expand.grid(\n",
    "        prior = prior.default,\n",
    "        laplace = laplace, \n",
    "        usekernel = usekernel.default, \n",
    "        usepoisson = usepoisson.default\n",
    "    ),\n",
    "\n",
    "    # Metrics\n",
    "    acc = getacc,\n",
    "    f1 = getF1\n",
    ")\n",
    "\n",
    "# Arrange model params by training accuracy\n",
    "results %>% arrange(desc(validation.f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Kernel\n",
    "Most of our variables are not normally distributed so we expect the latter to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <-  grid.search(\n",
    "    model.func = func,\n",
    "    data = data.reduced,\n",
    "    param.grid = expand.grid(\n",
    "        prior = prior.default,\n",
    "        laplace = laplace.default, \n",
    "        usekernel = usekernel, \n",
    "        usepoisson = usepoisson.default\n",
    "    ),\n",
    "\n",
    "    # Metrics\n",
    "    AUC = getAUC, \n",
    "    acc = getacc\n",
    ")\n",
    "\n",
    "# Arrange model params by training accuracy\n",
    "results %>% arrange(desc(validation.acc))\n",
    "\n",
    "params.data <- list(data=train.data %>% mutate(DEFAULT=as.factor(DEFAULT)))\n",
    "        params.grid <- param.grid %>% slice(i) %>% flatten()\n",
    "        params <- append(params.data, params.grid)\n",
    "        model.fit <- do.call(model.func, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <-  grid.search(\n",
    "    model.func = func,\n",
    "    data = data.reduced,\n",
    "    param.grid = expand.grid(\n",
    "        prior = prior.default,\n",
    "        laplace = laplace.default, \n",
    "        usekernel = usekernel.default, \n",
    "        usepoisson = usepoisson\n",
    "    ),\n",
    "\n",
    "    # Metrics\n",
    "    AUC = getAUC, \n",
    "    acc = getacc\n",
    ")\n",
    "\n",
    "# Arrange model params by training accuracy\n",
    "results %>% arrange(desc(validation.acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- grid.search(\n",
    "    model.func = func,\n",
    "    data = data.reduced,\n",
    "    param.grid = expand.grid(\n",
    "        prior = prior,\n",
    "        laplace = laplace, \n",
    "        usekernel = usekernel, \n",
    "        usepoisson = usepoisson\n",
    "    ),\n",
    "\n",
    "    # Metrics\n",
    "    acc = getacc,\n",
    "    f1 = getF1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "acc.top10 <- results %>%\n",
    "  arrange(desc(validation.acc)) %>%\n",
    "  head(10)\n",
    "\n",
    "options(width=150)\n",
    "print('best params by validation accuracy')\n",
    "print.data.frame(acc.top10)\n",
    "\n",
    "acc.topParams <- acc.top10 %>% slice(1) %>% flatten()\n",
    "acc.topModel <- build.model(func, data.reduced, acc.topParams)\n",
    "\n",
    "summary(acc.topModel)\n",
    "plot(acc.topModel)\n",
    "\n",
    "# Save Model\n",
    "file = paste(path, 'models', 'model_nb_acc.rda', sep='/')\n",
    "saveRDS(acc.topModel, file = file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "f1.top10 <- results %>%\n",
    "  arrange(desc(validation.f1)) %>%\n",
    "  head(10)\n",
    "\n",
    "options(width=150)\n",
    "print('best params by validation F1 score')\n",
    "print(f1.top10)\n",
    "\n",
    "f1.topParams <- f1.top10 %>% slice(1) %>% flatten()\n",
    "f1.topModel <- build.model(func, data.reduced, f1.topParams)\n",
    "\n",
    "summary(f1.topModel)\n",
    "plot(f1.topModel)\n",
    "\n",
    "# Save Model\n",
    "file = paste(path, 'models', 'model_nb_f1.rda', sep='/')\n",
    "saveRDS(f1.topModel, file = file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Means across all models')\n",
    "results %>% \n",
    "    select(training.acc, training.f1, validation.acc, validation.f1) %>%\n",
    "    replace_na(list(validation.f1=0, training.f1=0)) %>%\n",
    "    summarise_all(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "library(ROSE)\n",
    "library(e1071)\n",
    "\n",
    "# Load data\n",
    "balanced <- read.table(\"card_train_oversample.csv\", sep = \",\", header = T)\n",
    "bal.d <- balanced %>% select(-c(ID)) # Remove ID columns\n",
    "\n",
    "# Evaluation Metrics\n",
    "F1 <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, newdata=data, type='class')\n",
    "  \n",
    "  xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                factor(y.pred, levels=c(1, 0)))\n",
    "  \n",
    "  precision <- xtab[1,1] / sum(xtab[,1])\n",
    "  recall <- xtab[1,1] / sum(xtab[1,])\n",
    "  \n",
    "  return(2 * (precision * recall) / (precision + recall))\n",
    "}\n",
    "\n",
    "Accuracy <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, newdata = data, type = \"class\")\n",
    "  \n",
    "  xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                factor(y.pred, levels=c(1, 0)))\n",
    "  accuracy <- (xtab[1,1] + xtab[2,2]) / length(y)\n",
    "  \n",
    "  return (accuracy)\n",
    "}\n",
    "\n",
    "# Function to create and test models\n",
    "cvFunction <- function(fold, kernel, cost, gamma) {\n",
    "  training_fold <- bal.d[-fold, ]\n",
    "  val_fold <- bal.d[fold, ]\n",
    "  classifier <- svm(\n",
    "    DEFAULT ~.,\n",
    "    data = training_fold,\n",
    "    type = \"C-classification\",\n",
    "    kernel = kernel,\n",
    "    cost = cost,\n",
    "    gamma = gamma\n",
    "  )\n",
    "  \n",
    "  accuracy <- Accuracy(classifier, val_fold)\n",
    "  f1score <- F1(classifier, val_fold)\n",
    "  return(list(\"accuracy\" = accuracy, \"f1\" = f1score))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Kernel Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.type <- data.frame(Accuracy = numeric(), F1 = numeric())\n",
    "kernelTypes <- c(\"linear\", \"radial\", \"polynomial\", \"sigmoid\")\n",
    "for (type in kernelTypes) {\n",
    "  folds <- cut(seq(1,nrow(bal.d)), breaks = 10, labels = FALSE)\n",
    "  \n",
    "  accuracy <- 0\n",
    "  f1 <- 0\n",
    "  \n",
    "  for (i in 1:10) {\n",
    "    # Segment data by fold using the which() function\n",
    "    val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "    \n",
    "    # Put it into the cvFunction\n",
    "    res <- cvFunction(val.indexes, type, 0.1, 0.5)\n",
    "    \n",
    "    # Store each \n",
    "    accuracy <- accuracy + res$accuracy\n",
    "    f1 <- f1 + res$f1\n",
    "  }\n",
    "  \n",
    "  # Get mean\n",
    "  meanAcc <- accuracy / 10\n",
    "  meanF1 <- f1 / 10\n",
    "  \n",
    "  results.type <- rbind(results.type, c(meanAcc, meanF1))\n",
    "}\n",
    "results.type <- cbind(kernelTypes, results.type)\n",
    "names(results.type) <- c(\"Kernel\", \"Accuracy\", \"F1\")\n",
    "write.csv(results.type, \"SVM_results_type.csv\")\n",
    "results.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cost\n",
    "results.cost <- data.frame(Cost = numeric(), Accuracy = numeric(), F1 = numeric())\n",
    "for (cost in 10^(-2:2)) {\n",
    "  folds <- cut(seq(1,nrow(bal.d)), breaks = 10, labels = FALSE)\n",
    "  \n",
    "  accuracy <- 0\n",
    "  f1 <- 0\n",
    "  \n",
    "  for (i in 1:10) {\n",
    "    # Segment data by fold using the which() function\n",
    "    val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "    \n",
    "    # Put it into the cvFunction\n",
    "    res <- cvFunction(val.indexes, \"radial\", cost, 0.5)\n",
    "    \n",
    "    # Store each \n",
    "    accuracy <- accuracy + res$accuracy\n",
    "    f1 <- f1 + res$f1\n",
    "  }\n",
    "  \n",
    "  # Get mean\n",
    "  meanAcc <- accuracy / 10\n",
    "  meanF1 <- f1 / 10\n",
    "  \n",
    "  results.cost <- rbind(results.cost, c(cost, meanAcc, meanF1))\n",
    "}\n",
    "names(results.cost) <- c(\"Cost\", \"Accuracy\", \"F1\")\n",
    "write.csv(results,cost, \"SVM_results_cost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.gamma <- data.frame(Gamma = numeric(), Accuracy = numeric(), F1 = numeric())\n",
    "for (gamma in 2^(-2:2)) {\n",
    "  folds <- cut(seq(1,nrow(bal.d)), breaks = 10, labels = FALSE)\n",
    "  \n",
    "  accuracy <- 0\n",
    "  f1 <- 0\n",
    "  \n",
    "  for (i in 1:10) {\n",
    "    # Segment data by fold using the which() function\n",
    "    val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "    \n",
    "    # Put it into the cvFunction\n",
    "    res <- cvFunction(val.indexes, \"radial\", 0.1, gamma)\n",
    "    \n",
    "    # Store each \n",
    "    accuracy <- accuracy + res$accuracy\n",
    "    f1 <- f1 + res$f1\n",
    "  }\n",
    "  \n",
    "  # Get mean\n",
    "  meanAcc <- accuracy / 10\n",
    "  meanF1 <- f1 / 10\n",
    "  \n",
    "  results.gamma <- rbind(results.gamma, c(meanAcc, meanF1))\n",
    "}\n",
    "names(results.gamma) <- c(\"Gamma\", \"Accuracy\", \"F1\")\n",
    "write.csv(results.gamma, \"SVM_results_gamma.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search on Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search on parameteres\n",
    "gs <- list(type = c(\"polynomial\"), \n",
    "           cost = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100),\n",
    "           gamma = c(0, 0.25, 0.5)) %>% \n",
    "  cross_df()\n",
    "\n",
    "# Store results\n",
    "results.gs <- data.frame(Cost = numeric(), Gamma = numeric(), Accuracy = numeric(), F1 = numeric())\n",
    "\n",
    "for (i in 1:nrow(gs)) {\n",
    "  i.cost <- gs$cost[i]\n",
    "  i.gamma <- gs$gamma[i]\n",
    "  i.type <- gs$type[i]\n",
    "  print(paste(\"cost:\", i.cost, \", gamma:\", i.gamma))\n",
    "  \n",
    "  folds <- cut(seq(1,nrow(bal.d)), breaks = 10, labels = FALSE)\n",
    "  \n",
    "  accuracy <- 0\n",
    "  f1 <- 0\n",
    "  \n",
    "  for (i in 1:10) {\n",
    "    print(paste(\"cost:\", i.cost, \", gamma:\", i.gamma, \", iteration:\", i))\n",
    "    # Segment data by fold using the which() function\n",
    "    val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "    \n",
    "    # Put it into the cvFunction\n",
    "    res <- cvFunction(val.indexes, i.type, i.cost, i.gamma)\n",
    "    \n",
    "    # Store each \n",
    "    accuracy <- accuracy + res$accuracy\n",
    "    f1 <- f1 + res$f1\n",
    "  }\n",
    "  \n",
    "  # Get mean\n",
    "  meanAcc <- accuracy / 10\n",
    "  meanF1 <- f1 / 10\n",
    "  \n",
    "  print(paste(\"cost:\", i.cost, \", gamma:\", i.gamma, \", mean accuracy:\", meanAcc, \", mean F1:\", meanF1))\n",
    "  results.gs <- rbind(results.gs, c(i.cost, i.gamma, meanAcc, meanF1))\n",
    "}\n",
    "\n",
    "# View results\n",
    "names(results.gs) <- c(\"Cost\", \"Gamma\", \"Accuracy\", \"F1\")\n",
    "results.gs <- cbind(Kernel = \"polynomial\", results.gs)\n",
    "write.csv(results.gs, \"SVM_results.csv\")\n",
    "results.gs <- results.gs[order(-results.gs$F1),]\n",
    "head(results.gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the finalised model on the full train dataset \n",
    "set.seed(123)\n",
    "model.svm <- svm(\n",
    "  DEFAULT ~.,\n",
    "  data = bal.d,\n",
    "  type = \"C-classification\",\n",
    "  kernel = \"polynomial\",\n",
    "  cost = 60,\n",
    "  gamma = 0.25\n",
    ")\n",
    "\n",
    "## Save the model (for future reference)\n",
    "saveRDS(model.svm, file = \"model_svm.rda\")\n",
    "\n",
    "## To load the model, uncomment the following line\n",
    "model.svm <- readRDS(\"model_svm.rda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "F1 <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, newdata=data, type='class')\n",
    "  \n",
    "  xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                factor(y.pred, levels=c(1, 0)))\n",
    "  \n",
    "  precision <- xtab[1,1] / sum(xtab[,1])\n",
    "  recall <- xtab[1,1] / sum(xtab[1,])\n",
    "  \n",
    "  return(2 * (precision * recall) / (precision + recall))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set formula\n",
    "formula <- as.formula(paste(\"train.class ~ \", paste(labels[2:45], collapse = \"+\")))\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute RMSE\n",
    "RMSE.calc <- function(model, val.data) {\n",
    "  mr <- model$residuals\n",
    "  rmse_squared = 0\n",
    "  n <- nrow(val.data)\n",
    "  for (i in 1:n) {\n",
    "    rmse_squared = rmse_squared + (mr[i, 1])**2\n",
    "  }\n",
    "  rmse_squared = rmse_squared / (n - 1)\n",
    "  rmse = sqrt(rmse_squared)\n",
    "  rmse\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for Different Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 equally size folds\n",
    "folds <- cut(seq(1,nrow(initial.train.data)), breaks = 10, labels = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for different number of hidden layers (size)\n",
    "results.size <- data.frame(Size = numeric(), RMSE = numeric(), F1 = numeric())\n",
    "size <- 1\n",
    "while (size <= 21) {\n",
    "  # Initialise RMSE and F1\n",
    "  rmse <- 0\n",
    "  f1 <- 0\n",
    "  \n",
    "  # Perform 10 fold cross validation\n",
    "  for (i in 1:10) {\n",
    "    # Segment data by fold using the which() function\n",
    "    val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "    \n",
    "    # Split train and test data\n",
    "    val.data <- initial.train.data[val.indexes, ]\n",
    "    train.data <- initial.train.data[-val.indexes, ]\n",
    "    \n",
    "    train.class <- train.data[, 46]\n",
    "    \n",
    "    # Set seed and train model\n",
    "    set.seed(123)\n",
    "    model.nn <- nnet(formula, data = train.data, size = size, maxit = 1000, entropy = TRUE)\n",
    "    \n",
    "    # Calculate RMSE and Accuracy \n",
    "    rmse <- rmse + RMSE.calc(model.nn, val.data)\n",
    "    f1 <- f1 + F1(model.nn, val.data)\n",
    "  }\n",
    "  \n",
    "  # Get average RMSE and Accuracy\n",
    "  rmse <- rmse / 10\n",
    "  f1 <- f1 / 10\n",
    "  \n",
    "  # Add to results\n",
    "  results.size <- rbind(results.size, c(size, rmse, f1))\n",
    "  \n",
    "  size = size + 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "names(results.size) <- c(\"Size\", \"RMSE\", \"F1\")\n",
    "results.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for Different Decays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for different decays\n",
    "# Set default size = 10\n",
    "results.decay <- data.frame(Decay = numeric(), RMSE = numeric(), F1 = numeric())\n",
    "decay <- 0\n",
    "while (decay <= 1) {\n",
    "  # Initialise RMSE and F1\n",
    "  rmse <- 0\n",
    "  f1 <- 0\n",
    "  \n",
    "  # Perform 10 fold cross validation\n",
    "  for (i in 1:10) {\n",
    "    # Segment data by fold using the which() function\n",
    "    val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "    \n",
    "    # Split train and test data\n",
    "    val.data <- initial.train.data[val.indexes, ]\n",
    "    train.data <- initial.train.data[-val.indexes, ]\n",
    "    \n",
    "    train.class <- train.data[, 46]\n",
    "    \n",
    "    # Set seed and train model\n",
    "    set.seed(123)\n",
    "    model.nn <- nnet(formula, data = train.data, size = 10, maxit = 1000, decay = decay, entropy = TRUE)\n",
    "    \n",
    "    # Calculate RMSE and Accuracy \n",
    "    rmse <- rmse + RMSE.calc(model.nn, val.data)\n",
    "    f1 <- f1 + F1(model.nn, val.data)\n",
    "  }\n",
    "  \n",
    "  # Get average RMSE and Accuracy\n",
    "  rmse <- rmse / 10\n",
    "  f1 <- f1 / 10\n",
    "  \n",
    "  # Add to results\n",
    "  results.decay <- rbind(results.decay, c(decay, rmse, f1))\n",
    "  \n",
    "  decay = decay + 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "names(results.decay) <- c(\"Decay\", \"RMSE\", \"F1\")\n",
    "results.decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "ggplot(results.decay, aes(x = Decay, y = Value)) + \n",
    "  geom_line(aes(y = RMSE), color = \"darkred\") + \n",
    "  geom_line(aes(y = Accuracy), color=\"steelblue\", linetype=\"twodash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "results.gs <- data.frame(Size = numeric(), Decay = numeric(), F1 = numeric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a named list of parameter values\n",
    "# Intervals for decay were decreased from 0.1 during the testing phase to 0.025 increments to more closely\n",
    "# tune the decay.\n",
    "gs <- list(size = c(16, 17, 18, 19, 20, 21),\n",
    "           decay = c(0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2)) %>% \n",
    "  cross_df() # Convert to data frame grid\n",
    "\n",
    "for (i in 1:nrow(gs)) {\n",
    "  i.size <- gs$size[i]\n",
    "  i.decay <- gs$decay[i]\n",
    "  \n",
    "  # Initialise F1 score\n",
    "  f1 <- 0\n",
    "  \n",
    "  # Perform 10 fold cross validation\n",
    "  for (i in 1:10) {\n",
    "    # Segment data by fold using the which() function\n",
    "    val.indexes <- which(folds == i, arr.ind = TRUE)\n",
    "    \n",
    "    # Split train and test data\n",
    "    val.data <- initial.train.data[val.indexes, ]\n",
    "    train.data <- initial.train.data[-val.indexes, ]\n",
    "    \n",
    "    train.class <- train.data[, 46]\n",
    "    \n",
    "    # Set seed and train model\n",
    "    set.seed(123)\n",
    "    model.nn <- nnet(formula, data = train.data, size = i.size, maxit = 1000, decay = i.decay, entropy = TRUE)\n",
    "    \n",
    "    # Calculate F1\n",
    "    f1 <- f1 + F1(model.nn, val.data)\n",
    "  }\n",
    "  \n",
    "  f1 <- f1 / 10\n",
    "  \n",
    "  # Add to results\n",
    "  results.gs <- rbind(results.gs, c(i.size, i.decay, f1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "names(results.gs) <- c(\"Size\", \"Decay\", \"F1\")\n",
    "results.gs <- results.gs[order(-results.gs$F1),]\n",
    "head(results.gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the finalised model on the full train dataset \n",
    "# (size = 20, decay = 0, entropy = TRUE, maxit = 1000)\n",
    "set.seed(123)\n",
    "train.class <- initial.train.data$DEFAULT\n",
    "model.nn <- nnet(formula, data = initial.train.data, size = 20, decay = 0, maxit = 1000, entropy = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model (for future reference)\n",
    "saveRDS(model.nn, file = \"model_nn.rda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Logistic Regression Model\n",
    "lr.model <- readRDS(\"model_lr.rda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions for Logistic Regression\n",
    "acc <- function(model, data, optcut) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, newdata=data, type='response')\n",
    "  y.pred <- ifelse(y.pred < optcut ,0,1)\n",
    "  \n",
    "  return(mean(y == y.pred))\n",
    "}\n",
    "\n",
    "\n",
    "AUC <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, newdata=data, type='response')\n",
    "  \n",
    "  M <- plotROC(y, y.pred, returnSensitivityMat = TRUE)\n",
    "  x <- M[,1]\n",
    "  y <- M[,2]\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "F1 <- function(model, data, optcut) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, newdata=data, type='response')\n",
    "  y.pred <- ifelse(y.pred < optcut ,0,1)\n",
    "  \n",
    "  xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                factor(y.pred, levels=c(1, 0)))\n",
    "  \n",
    "  precision <- xtab[1,1] / sum(xtab[,1])\n",
    "  recall <- xtab[1,1] / sum(xtab[1,])\n",
    "  \n",
    "  return(2 * (precision * recall) / (precision + recall))\n",
    "}\n",
    "\n",
    "\n",
    "gain <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, data, type = 'response')\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  total.positive = nrow(data[data$DEFAULT == 1,])\n",
    "  \n",
    "  # Calculate gain\n",
    "  df.gains <- data.frame(Percentile = numeric(), Gain = numeric())\n",
    "  df.gains <- rbind(df.gains, c(0, 0))\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    g <- n / total.positive\n",
    "    df.gains <- rbind(df.gains, c(perc, g))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.gains) <- c(\"Percentile\", \"Gain\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.gains$CumGain <- cumsum(df.gains$Gain)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.gains$Percentile\n",
    "  y <- df.gains$CumGain\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.gains$Percentile,df.gains$CumGain,type=\"l\",xlab=\"Percentile\",ylab=\"Gain\", main = \"Cumulative Gain Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "lift <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, data, type = 'response')\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  perc.positive = nrow(data[data$DEFAULT == 1,]) / nrow(data)\n",
    "  \n",
    "  # Calculate lift\n",
    "  df.lift <- data.frame(Percentile = numeric(), Lift = numeric())\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    p <- n / nrow(df[df$quantile == (11-i),])\n",
    "    l <- p / perc.positive\n",
    "    df.lift <- rbind(df.lift, c(perc, l))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.lift) <- c(\"Percentile\", \"Lift\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.lift$CumLift <- cumsum(df.lift$Lift)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.lift$Percentile\n",
    "  y <- df.lift$Lift\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.lift$Percentile, df.lift$Lift,type=\"l\",xlab=\"Percentile\",ylab=\"Lift\", main = \"Lift Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Test Set\n",
    "acc(lr.model, test.data, optcut.final)\n",
    "F1(lr.model, test.data, optcut.final)\n",
    "AUC(lr.model, test.data)\n",
    "gain(lr.model, test.data)\n",
    "lift(lr.model, test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions for BOTH Decision Tree and Random Forest\n",
    "acc <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='class')\n",
    "    # y.pred = predict(model, newdata=data)\n",
    "\n",
    "    return(mean(y == y.pred))\n",
    "}\n",
    "\n",
    "\n",
    "AUC <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='prob')[,2]\n",
    "    \n",
    "    M <- plotROC(y, y.pred, returnSensitivityMat = TRUE)\n",
    "    x <- M[,1]\n",
    "    y <- M[,2]\n",
    "\n",
    "    W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "    H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "    \n",
    "    return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "F1 <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='class')\n",
    "\n",
    "    xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                  factor(y.pred, levels=c(1, 0)))\n",
    "    \n",
    "    precision <- xtab[1,1] / sum(xtab[,1])\n",
    "    recall <- xtab[1,1] / sum(xtab[1,])\n",
    "\n",
    "    return(2 * (precision * recall) / (precision + recall))\n",
    "}\n",
    "\n",
    "\n",
    "gain <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, data, type = 'prob')[,2]\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  df$actual <- ifelse(df$actual == 1, 0, 1)\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  total.positive = nrow(data[data$DEFAULT == 1,])\n",
    "  \n",
    "  # Calculate gain\n",
    "  df.gains <- data.frame(Percentile = numeric(), Gain = numeric())\n",
    "  df.gains <- rbind(df.gains, c(0, 0))\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    g <- n / total.positive\n",
    "    df.gains <- rbind(df.gains, c(perc, g))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.gains) <- c(\"Percentile\", \"Gain\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.gains$CumGain <- cumsum(df.gains$Gain)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.gains$Percentile\n",
    "  y <- df.gains$CumGain\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.gains$Percentile,df.gains$CumGain,type=\"l\",xlab=\"Percentile\",ylab=\"Gain\", main = \"Cumulative Gain Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "lift <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, data, type = 'prob')[,2]\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  df$actual <- ifelse(df$actual == 1, 0, 1)\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  perc.positive = nrow(data[data$DEFAULT == 1,]) / nrow(data)\n",
    "  \n",
    "  # Calculate lift\n",
    "  df.lift <- data.frame(Percentile = numeric(), Lift = numeric())\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    p <- n / nrow(df[df$quantile == (11-i),])\n",
    "    l <- p / perc.positive\n",
    "    df.lift <- rbind(df.lift, c(perc, l))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.lift) <- c(\"Percentile\", \"Lift\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.lift$CumLift <- cumsum(df.lift$Lift)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.lift$Percentile\n",
    "  y <- df.lift$Lift\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.lift$Percentile, df.lift$Lift,type=\"l\",xlab=\"Percentile\",ylab=\"Lift\", main = \"Lift Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Test Set\n",
    "acc(model.CART.pruned, test.data)\n",
    "F1(model.CART.pruned, test.data)\n",
    "lift(model.CART.pruned, test.data)\n",
    "gain(model.CART.pruned, test.data)\n",
    "AUC(model.CART.pruned, test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function Same as Decision Tree\n",
    "# Evaluate Test Set\n",
    "acc(model.rf, test.data)\n",
    "F1(model.rf, test.data)\n",
    "lift(model.rf, test.data)\n",
    "gain(model.rf, test.data)\n",
    "AUC(model.rf, test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions for Naive Bayes\n",
    "acc <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='class')\n",
    "    # y.pred = predict(model, newdata=data)\n",
    "\n",
    "    return(mean(y == y.pred))\n",
    "}\n",
    "\n",
    "\n",
    "AUC <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='prob')[,2]\n",
    "    \n",
    "    M <- plotROC(y, y.pred, returnSensitivityMat = TRUE)\n",
    "    x <- M[,1]\n",
    "    y <- M[,2]\n",
    "\n",
    "    W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "    H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "    \n",
    "    return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "F1 <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='class')\n",
    "    # y.pred = predict(model, newdata=data)\n",
    "\n",
    "    xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                  factor(y.pred, levels=c(1, 0)))\n",
    "    \n",
    "    precision <- xtab[1,1] / sum(xtab[,1])\n",
    "    recall <- xtab[1,1] / sum(xtab[1,])\n",
    "\n",
    "    return(2 * (precision * recall) / (precision + recall))\n",
    "}\n",
    "\n",
    "\n",
    "gain <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, data, type = 'prob')[,2]\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  total.positive = nrow(data[data$DEFAULT == 1,])\n",
    "  \n",
    "  # Calculate gain\n",
    "  df.gains <- data.frame(Percentile = numeric(), Gain = numeric())\n",
    "  df.gains <- rbind(df.gains, c(0, 0))\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    g <- n / total.positive\n",
    "    df.gains <- rbind(df.gains, c(perc, g))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.gains) <- c(\"Percentile\", \"Gain\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.gains$CumGain <- cumsum(df.gains$Gain)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.gains$Percentile\n",
    "  y <- df.gains$CumGain\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.gains$Percentile,df.gains$CumGain,type=\"l\",xlab=\"Percentile\",ylab=\"Gain\", main = \"Cumulative Gain Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "lift <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, data, type = 'prob')[,2]\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  perc.positive = nrow(data[data$DEFAULT == 1,]) / nrow(data)\n",
    "  \n",
    "  # Calculate lift\n",
    "  df.lift <- data.frame(Percentile = numeric(), Lift = numeric())\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    p <- n / nrow(df[df$quantile == (11-i),])\n",
    "    l <- p / perc.positive\n",
    "    df.lift <- rbind(df.lift, c(perc, l))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.lift) <- c(\"Percentile\", \"Lift\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.lift$CumLift <- cumsum(df.lift$Lift)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.lift$Percentile\n",
    "  y <- df.lift$Lift\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.lift$Percentile, df.lift$Lift,type=\"l\",xlab=\"Percentile\",ylab=\"Lift\", main = \"Lift Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.models <- function(models, data, metrics) {\n",
    "    result <- data.frame(matrix(nrow=0, ncol=length(metrics)))\n",
    "    \n",
    "    for (model.name in names(models)) {\n",
    "        model <- models[[model.name]]\n",
    "        model.metrics <- metrics %>% sapply((function(f) f(model, data)))\n",
    "        result <- rbind(result, model.metrics)\n",
    "    }\n",
    "\n",
    "    result <- cbind(names(models), result)\n",
    "    names(result) <- append('model', names(metrics))\n",
    "    return(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Test Set\n",
    "data.test <- read.csv(paste(path, 'data', 'card_test.csv', sep='/')) %>% as_tibble()\n",
    "\n",
    "models <- list(\n",
    "    naive.bayes.acc = readRDS(paste(path, 'models', 'model_nb_acc.rda', sep='/')),\n",
    "    naive.bayes.f1 = readRDS(paste(path, 'models', 'model_nb_f1.rda', sep='/')),\n",
    "    naive.bayes.auc = readRDS(paste(path, 'models', 'model_nb_AUC.rda', sep='/'))\n",
    ")\n",
    "\n",
    "metrics <- list(\n",
    "    accuracy = acc,\n",
    "    f1 = F1,\n",
    "    AUROC = AUC,\n",
    "    lift = lift,\n",
    "    gain = gain\n",
    ")\n",
    "\n",
    "evaluate.models(models = models, data = data.test, metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function for SVM\n",
    "acc <- function(model, data) {\n",
    "    y = as.factor(data$DEFAULT)\n",
    "    y.pred = predict(model, newdata=data)\n",
    "\n",
    "    return(mean(y == y.pred))\n",
    "}\n",
    "\n",
    "AUC <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = attr(predict(model, data, probability=TRUE), 'probabilities')[,1]\n",
    "  \n",
    "    M <- plotROC(y, y.pred, returnSensitivityMat = TRUE)\n",
    "    x <- M[,1]\n",
    "    y <- M[,2]\n",
    "\n",
    "    W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "    H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "    \n",
    "    return(W %*% H)\n",
    "}\n",
    "\n",
    "F1 <- function(model, data) {\n",
    "    y = as.factor(data$DEFAULT)\n",
    "    y.pred = predict(model, newdata=data)\n",
    "    # y.pred = predict(model, newdata=data)\n",
    "\n",
    "    xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                  factor(y.pred, levels=c(1, 0)))\n",
    "    \n",
    "    precision <- xtab[1,1] / sum(xtab[,1])\n",
    "    recall <- xtab[1,1] / sum(xtab[1,])\n",
    "\n",
    "    return(2 * (precision * recall) / (precision + recall))\n",
    "}\n",
    "\n",
    "gain <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = attr(predict(model, data, probability=TRUE), 'probabilities')[,1]\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  total.positive = nrow(data[data$DEFAULT == 1,])\n",
    "  \n",
    "  # Calculate gain\n",
    "  df.gains <- data.frame(Percentile = numeric(), Gain = numeric())\n",
    "  df.gains <- rbind(df.gains, c(0, 0))\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    g <- n / total.positive\n",
    "    df.gains <- rbind(df.gains, c(perc, g))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.gains) <- c(\"Percentile\", \"Gain\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.gains$CumGain <- cumsum(df.gains$Gain)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.gains$Percentile\n",
    "  y <- df.gains$CumGain\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.gains$Percentile,df.gains$CumGain,type=\"l\",xlab=\"Percentile\",ylab=\"Gain\", main = \"Cumulative Gain Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}\n",
    "\n",
    "lift <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = attr(predict(model, data, probability=TRUE), 'probabilities')[,1]\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  perc.positive = nrow(data[data$DEFAULT == 1,]) / nrow(data)\n",
    "  \n",
    "  # Calculate lift\n",
    "  df.lift <- data.frame(Percentile = numeric(), Lift = numeric())\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    p <- n / nrow(df[df$quantile == (11-i),])\n",
    "    l <- p / perc.positive\n",
    "    df.lift <- rbind(df.lift, c(perc, l))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.lift) <- c(\"Percentile\", \"Lift\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.lift$CumLift <- cumsum(df.lift$Lift)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.lift$Percentile\n",
    "  y <- df.lift$Lift\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.lift$Percentile, df.lift$Lift,type=\"l\",xlab=\"Percentile\",ylab=\"Lift\", main = \"Lift Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}\n",
    "\n",
    "evaluate.models <- function(models, data, metrics) {\n",
    "    result <- data.frame(matrix(nrow=0, ncol=length(metrics)))\n",
    "    \n",
    "    for (model.name in names(models)) {\n",
    "        model <- models[[model.name]]\n",
    "        model.metrics <- metrics %>% sapply((function(f) f(model, data)))\n",
    "        result <- rbind(result, model.metrics)\n",
    "    }\n",
    "\n",
    "    result <- cbind(names(models), result)\n",
    "    names(result) <- append('model', names(metrics))\n",
    "    return(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Test Set\n",
    "data.test <- read.csv(paste('data', 'card_test.csv', sep='/')) %>% as_tibble()\n",
    "\n",
    "models <- list(\n",
    "    naive.bayes.acc = readRDS(paste(path, 'models', 'model_svm.rda', sep='/'))\n",
    ")\n",
    "\n",
    "metrics <- list(\n",
    "    accuracy = acc,\n",
    "    f1 = F1,\n",
    "    AUROC = AUC,\n",
    "    lift = lift,\n",
    "    gain = gain\n",
    ")\n",
    "\n",
    "evaluate.models(models = models, data = data.test, metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions for Neural Network\n",
    "acc <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='class')\n",
    "    return(mean(y == y.pred))\n",
    "}\n",
    "\n",
    "\n",
    "AUC <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='raw')\n",
    "    \n",
    "    M <- plotROC(y, y.pred, returnSensitivityMat = TRUE)\n",
    "    x <- M[,1]\n",
    "    y <- M[,2]\n",
    "\n",
    "    W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "    H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "    \n",
    "    return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "F1 <- function(model, data) {\n",
    "    y = data$DEFAULT\n",
    "    y.pred = predict(model, newdata=data, type='class')\n",
    "\n",
    "    xtab <- table(factor(y, levels=c(1, 0)), \n",
    "                  factor(y.pred, levels=c(1, 0)))\n",
    "    \n",
    "    precision <- xtab[1,1] / sum(xtab[,1])\n",
    "    recall <- xtab[1,1] / sum(xtab[1,])\n",
    "\n",
    "    return(2 * (precision * recall) / (precision + recall))\n",
    "}\n",
    "\n",
    "\n",
    "gain <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, data, type = 'raw')\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  total.positive = nrow(data[data$DEFAULT == 1,])\n",
    "  \n",
    "  # Calculate gain\n",
    "  df.gains <- data.frame(Percentile = numeric(), Gain = numeric())\n",
    "  df.gains <- rbind(df.gains, c(0, 0))\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    g <- n / total.positive\n",
    "    df.gains <- rbind(df.gains, c(perc, g))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.gains) <- c(\"Percentile\", \"Gain\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.gains$CumGain <- cumsum(df.gains$Gain)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.gains$Percentile\n",
    "  y <- df.gains$CumGain\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.gains$Percentile,df.gains$CumGain,type=\"l\",xlab=\"Percentile\",ylab=\"Gain\", main = \"Cumulative Gain Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}\n",
    "\n",
    "\n",
    "lift <- function(model, data) {\n",
    "  y = data$DEFAULT\n",
    "  y.pred = predict(model, data, type = 'raw')\n",
    "  \n",
    "  # Combine the actual and pred\n",
    "  df <- as.data.frame(cbind(y, y.pred))\n",
    "  names(df) <- c(\"actual\", \"pred\")\n",
    "  \n",
    "  # Order the prediction scores in descending order\n",
    "  df <- df[order(-df$pred),]\n",
    "  \n",
    "  # Split into deciles\n",
    "  df <- df %>% mutate(quantile = ntile(pred, 10))\n",
    "  \n",
    "  # Get total no. of positive instances\n",
    "  perc.positive = nrow(data[data$DEFAULT == 1,]) / nrow(data)\n",
    "  \n",
    "  # Calculate lift\n",
    "  df.lift <- data.frame(Percentile = numeric(), Lift = numeric())\n",
    "  perc = 0.1\n",
    "  for (i in 1:10) {\n",
    "    n <- sum(df[df$quantile == (11-i),]$actual) # number of positive instances\n",
    "    p <- n / nrow(df[df$quantile == (11-i),])\n",
    "    l <- p / perc.positive\n",
    "    df.lift <- rbind(df.lift, c(perc, l))\n",
    "    perc <- perc + 0.1\n",
    "  }\n",
    "  names(df.lift) <- c(\"Percentile\", \"Lift\")\n",
    "  \n",
    "  # Calculate cumulative gain\n",
    "  df.lift$CumLift <- cumsum(df.lift$Lift)\n",
    "  \n",
    "  # Calculate and return area under cumulative gain chart\n",
    "  x <- df.lift$Percentile\n",
    "  y <- df.lift$Lift\n",
    "  \n",
    "  W <- x[2:length(x)] - x[1:(length(x)-1)]\n",
    "  H <- (y[2:length(x)] + y[1:(length(x)-1)]) / 2\n",
    "  \n",
    "  # Plot graph\n",
    "  plot(df.lift$Percentile, df.lift$Lift,type=\"l\",xlab=\"Percentile\",ylab=\"Lift\", main = \"Lift Chart\")\n",
    "  \n",
    "  return(W %*% H)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Test Set\n",
    "data.test <- read.csv(paste(path, 'data', 'card_test.csv', sep='/')) %>% as_tibble()\n",
    "\n",
    "models <- list(\n",
    "    neural.network = readRDS(paste(path, 'models', 'model_nn.rda', sep='/'))\n",
    ")\n",
    "\n",
    "metrics <- list(\n",
    "    accuracy = acc,\n",
    "    f1 = F1,\n",
    "    AUROC = AUC,\n",
    "    lift = lift,\n",
    "    gain = gain\n",
    ")\n",
    "\n",
    "evaluate.models(models = models, data = data.test, metrics = metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
